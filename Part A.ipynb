{"cells":[{"cell_type":"markdown","metadata":{"id":"9JjlU61sSbqt"},"source":["We will start by installing the previous version of Numpy to be able to run all the packages correctly. Please restart the runtime after this command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZTDYxSjbHlg"},"outputs":[],"source":["!pip install numpy==1.23.1"]},{"cell_type":"markdown","metadata":{"id":"2TvyemfDlDmu"},"source":["# Lab 7 - Social Media Processing\n","\n","This notebook shows how to use HuggingFace's package to import and train regression models to assess humor rating in social media posts in English (SemEval2021: HaHackathon: Detecting and Rating Humor and Offense https://competitions.codalab.org/competitions/27446, **Task-1b**). \n","\n","Detection of humour, especially in social media posts, poses a linguistic challenge to NLP, due to the noise, figurative language, contextuality and subjectivity. You will hence try different methods to address those challenges such as preprocessing, data augmentation, ensembling and multi-task learning.\n","\n","We will download and unzip the data from here: http://smash.inf.ed.ac.uk/hahackathon_data/hahackathon_data.zip. \n","\n","\n","We recommend you to do this lab on a Colab TPU provided by Google."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vkzoj2bRu3Ky"},"outputs":[],"source":["!wget http://smash.inf.ed.ac.uk/hahackathon_data/hahackathon_data.zip\n","!unzip '/content/hahackathon_data.zip'"]},{"cell_type":"markdown","metadata":{"id":"aa3hj8h05WZL"},"source":["First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece), as well as some helper libraries, with the following commands."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"modern-olympus"},"outputs":[],"source":["!pip install -q transformers\n","!pip install -q sentencepiece\n","!pip install -q  ipywidgets\n","!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85fRpTxP-W8L"},"outputs":[],"source":["import keras\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","\n","from keras.layers.core import Dense\n","from keras.layers import Input, GlobalAveragePooling1D\n","from keras.models import Model\n","from keras import backend as K"]},{"cell_type":"markdown","source":["We define the fix seed method to be able to introduce variety into ensembling models."],"metadata":{"id":"yLbYtQNndziN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CedBVCKMyq9d"},"outputs":[],"source":["def set_random_seed(seed=123):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","set_random_seed()"]},{"cell_type":"markdown","metadata":{"id":"Zj_jHh2-ggC3"},"source":["## Regression with BERT\n","\n","We will use the [DistilBert](https://arxiv.org/abs/1910.01108v4) model and its Tokeniser following the preprocessing code from Lab 6."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySbyGAsF-tNH"},"outputs":[],"source":["from transformers import DistilBertTokenizer\n","import tqdm\n","\n","# we will pad to 128 subword tokens\n","PAD_LENGTH = 128\n","bert = 'distilbert-base-uncased'\n","BATCH_SIZE = 512\n","EPOCHS =10\n","\n","# Defining DistilBERT tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained(bert, do_lower_case=True, add_special_tokens=True, \n","                                                max_length=PAD_LENGTH, padding='max_length', truncation=True)\n","\n","def tokenize(sentences, tokenizer, pad_length=PAD_LENGTH):\n","    if type(sentences) == str:\n","        inputs = tokenizer.encode_plus(sentences, add_special_tokens=True, max_length=pad_length, padding='max_length', truncation=True,  \n","                                             return_attention_mask=True, return_token_type_ids=True)\n","        return np.asarray(inputs['input_ids'], dtype='int32'), np.asarray(inputs['attention_mask'], dtype='int32'), np.asarray(inputs['token_type_ids'], dtype='int32')\n","    input_ids, input_masks, input_segments = [],[],[]\n","    for sentence in sentences:\n","        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=pad_length, padding='max_length', truncation=True, \n","                                             return_attention_mask=True, return_token_type_ids=True)\n","        input_ids.append(inputs['input_ids'])\n","        input_masks.append(inputs['attention_mask'])\n","        input_segments.append(inputs['token_type_ids'])        \n"," \n","    return (np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32'))\n"]},{"cell_type":"markdown","metadata":{"id":"WaEJpImWjBdB"},"source":["Let's read the data using Pandas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kHK7eTIBAbk"},"outputs":[],"source":["import pandas as pd\n","\n","# Load data with only the necessary columns\n","train_df = pd.read_csv('hahackathon data/train.csv', usecols = ['text','humor_rating','offense_rating'])\n","test_df = pd.read_csv('hahackathon data/test.csv', usecols = ['text','humor_rating','offense_rating'])\n","\n","# Drop the Nans\n","train_df = train_df.dropna()\n","test_df = test_df.dropna()"]},{"cell_type":"markdown","metadata":{"id":"x54kpiRejuQd"},"source":["Let's check a couple of examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRSqyozoERV7"},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AO-g6KuyBAWG"},"outputs":[],"source":["# Get the post text\n","train_examples_list = train_df['text'].tolist()\n","test_examples_list = test_df['text'].tolist()\n","\n","# Get the humour rating for the regression task (we normalise, the values are between 0 and 5)\n","train_targets_list = (train_df['humor_rating']/5).tolist()\n","test_targets_list = (test_df['humor_rating']/5).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTg5t7uvtdkd"},"outputs":[],"source":["def get_bert_inputs(examples_list, targets):\n","  input_ids=list()\n","  attention_masks=list()\n","\n","  bert_inp=tokenize(examples_list, tokenizer)\n","  input_ids = bert_inp[0]\n","  attention_masks = bert_inp[1]\n","\n","  targets = np.array(targets)\n","\n","  return input_ids, attention_masks, targets\n","\n","train_input_ids, train_attention_masks, train_targets = get_bert_inputs(train_examples_list, train_targets_list)\n","test_input_ids, test_attention_masks, test_targets = get_bert_inputs(test_examples_list, test_targets_list)\n"]},{"cell_type":"markdown","metadata":{"id":"7oNjOY4rl6WR"},"source":["**Task 1: Build a neural bag of words model using DistilBERT embeddings and the sigmoid activation on the output layer for the regression task.**\n","\n","Investigate its performance using the Mean Squared Error (MSE) metric. We will use this metric as the loss function as well. We will also use the Adam optimiser with `learning_rate=2e-5`. This code is already provided.\n","\n","*Hint*: You can reuse the code from Lab 6 on Transfer Learning (Model 2)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALw1nMjNKTgn"},"outputs":[],"source":["from transformers import TFDistilBertModel, DistilBertConfig\n","import tensorflow as tf\n"," \n","\n","class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n","    def call(self, x, mask=None):\n","        if mask != None:\n","            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n","        else:\n","            return super().call(x)\n","\n","def get_BERT_layer():\n","  distil_bert = 'distilbert-base-uncased'\n","  config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n","  config.output_hidden_states = False\n","  \n","  return TFDistilBertModel.from_pretrained(distil_bert, config = config)\n","\n","def create_regression_BoW_bert():\n","  # Your code goes here\n","\n","  return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs=[out_reg])\n","\n","def get_model(use_tpu=True, use_gpu=False, learning_rate=2e-5):\n","  if use_tpu:\n","    # Create distribution strategy\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","\n","    # Create model on TPU:\n","    with strategy.scope():\n","      model = create_regression_BoW_bert()\n","      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","      model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.losses.MeanSquaredError()])\n","  elif use_gpu:\n","    device_name = tf.test.gpu_device_name()\n","    print(device_name)\n","    with tf.device('/device:GPU:0'):\n","      model = create_regression_BoW_bert()\n","      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","      model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.losses.MeanSquaredError()])\n","  else:\n","    model = create_regression_BoW_bert()\n","    model.compile(optimizer='adam',\n","                loss='mse',\n","                metrics=[tf.keras.losses.MeanSquaredError()])\n","  return model\n","\n","\n","model = get_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1677766725985,"user":{"displayName":"julia ive","userId":"11396706016758891847"},"user_tz":0},"id":"wpoibWdGQJA-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2098086-f1e6-48a5-d3f9-d659e4e3c801"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_token (InputLayer)       [(None, 128)]        0           []                               \n","                                                                                                  \n"," masked_token (InputLayer)      [(None, 128)]        0           []                               \n","                                                                                                  \n"," tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_token[0][0]',            \n"," BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n","                                one, 128, 768),                                                   \n","                                 hidden_states=None                                               \n","                                , attentions=None)                                                \n","                                                                                                  \n"," global_average_pooling1d_maske  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n"," d (GlobalAveragePooling1DMaske                                                                   \n"," d)                                                                                               \n","                                                                                                  \n"," dense (Dense)                  (None, 16)           12304       ['global_average_pooling1d_masked\n","                                                                 [0][0]']                         \n","                                                                                                  \n"," out_reg (Dense)                (None, 1)            17          ['dense[0][0]']                  \n","                                                                                                  \n","==================================================================================================\n","Total params: 66,375,201\n","Trainable params: 66,375,201\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHz1pJyV50cM"},"outputs":[],"source":["history = model.fit([train_input_ids, train_attention_masks],\n","                    train_targets,\n","                    epochs=EPOCHS,\n","                    batch_size=BATCH_SIZE,\n","                    verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"ILYZoU8bDskk"},"source":["We evaluate our model on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C75iSsedcRX"},"outputs":[],"source":["results = model.evaluate([test_input_ids,test_attention_masks], test_targets)\n","print('Test loss:', results[0])\n","print('Test MSE:', results[1])"]},{"cell_type":"markdown","metadata":{"id":"aa0sSeJE0UwU"},"source":["Get the array of predictions here so that you can plot the outputs later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpXK5mAs38US"},"outputs":[],"source":["preds = model.predict(\n","      [test_input_ids,test_attention_masks],\n","      batch_size=None,\n","      verbose=\"auto\",\n","      steps=None,\n","      callbacks=None,\n","      max_queue_size=10,\n","      workers=1,\n","      use_multiprocessing=False)\n","\n","preds = np.array(preds).flatten()"]},{"cell_type":"markdown","metadata":{"id":"_ll2c9dIPFdQ"},"source":["## Predictive Distribution\n","\n","We compute min, max and mean for the golden and predicted humour ratings. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9h_-3pxdNM9"},"outputs":[],"source":["min(preds), max(preds), preds.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Op_zenYZFNm8"},"outputs":[],"source":["min(test_targets), max(test_targets), test_targets.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7KO14nTkFUvq"},"outputs":[],"source":["pd.Series(preds).hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuRkmr0HPFdR"},"outputs":[],"source":["pd.Series(test_targets).hist()"]},{"cell_type":"markdown","metadata":{"id":"rt8ufAjVFjCf"},"source":["Next, we plot the true vs predicted humour grade for our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkTird8tFxrY"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","def get_pred_true_plot(preds, labels, title):\n","    limits = [labels.min(), labels.max()]\n","    fig, ax = plt.subplots()\n","    fig.set_dpi(150)\n","    ax.set_title(title)\n","    ax.scatter(labels, preds, marker='.')\n","    ax.plot(limits, limits, color=\"gray\", linestyle=\":\")\n","    ax.set_xlabel('True Humour Grade')\n","    ax.set_ylabel('Predicted Humour Grade')\n","    sns.regplot(x=labels, y=preds, ax=ax, scatter_kws={\"s\": 5})\n","    plt.show()\n","\n","get_pred_true_plot(preds, test_targets, 'True vs Predicted Humour Grade for DistilBERT Model')"]},{"cell_type":"markdown","metadata":{"id":"ves524STEEeR"},"source":["Our regressor tends to smooth down the extreme rating values to make them closer to the mean."]},{"cell_type":"markdown","metadata":{"id":"-LS3Oox93a4x"},"source":["# Feature Engineering & Data Augmentation\n","\n","**Task 2: Preprocess the textual data with the Ekphrasis library following the standard pipeline https://github.com/cbaziotis/ekphrasis#text-pre-processing-pipeline. How does this affect the performance?**"]},{"cell_type":"markdown","metadata":{"id":"ZOhLQtu_HZgq"},"source":["*Hint*: You might not want to annotate terms in order to keep the same length of the input sentences (for this, do not use the parameter `annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\", 'emphasis', 'censored'}`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTVPfG-43ZSe"},"outputs":[],"source":["!pip install -q ekphrasis\n","!pip3 install -q emoji==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_KBBFNn30Cp"},"outputs":[],"source":["# Your code goes here\n","\n","\n","new_train_examples_list = [\" \".join(text_processor.pre_process_doc(example)) for example in train_examples_list]\n","new_test_examples_list = [\" \".join(text_processor.pre_process_doc(example)) for example in test_examples_list]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgBytRATYCJl"},"outputs":[],"source":["print(\"Original Text:\", train_examples_list[0])\n","print(\"Preprocessed Text:\", new_train_examples_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16Vk_2Km31pB"},"outputs":[],"source":["train_input_ids, train_attention_masks, train_targets = get_bert_inputs(new_train_examples_list, train_targets_list)\n","test_input_ids, test_attention_masks, test_targets = get_bert_inputs(new_test_examples_list, test_targets_list)\n"," \n","model = get_model()\n","history = model.fit([train_input_ids,train_attention_masks],\n","                    train_targets,\n","                    epochs=EPOCHS,\n","                    batch_size=BATCH_SIZE,\n","                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jq1skI4nLEJl"},"outputs":[],"source":["results = model.evaluate([test_input_ids,test_attention_masks], test_targets)\n","print('Test loss:', results[0])\n","print('Test MSE:', results[1])"]},{"cell_type":"markdown","metadata":{"id":"DKzuPMLiQWL6"},"source":["Your results may be different depending on the implementation but typically special preprocessing does not drastically change the performance for this task."]},{"cell_type":"markdown","metadata":{"id":"rrfXrLUm36XZ"},"source":["**Task 3: Augment the training data twice by changing the original data via two methods from the Nlpaug (https://github.com/makcedward/nlpaug) library: (a) synonym replacement from WordNet; (b) deletion of random words. Comment on which method gives the best performance.**\n","\n","*Hint*: Use the Synonym Augmenter and Random Word Augmenter (Delete word randomly) classes as follows:\n","```\n","aug = naw.SynonymAug(aug_src='wordnet')\n","augmented_text = aug.augment(text)\n","\n","aug = naw.RandomWordAug()\n","augmented_text = aug.augment(text)\n","```\n","\n","\n","\n","\n","\n","For more examples check https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGBjZmyj37ZA"},"outputs":[],"source":["!pip install nlpaug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ke50JombOY-s"},"outputs":[],"source":["# Get the data again to apply augmentation\n","train_examples_list = train_df['text'].tolist()\n","train_targets_list = (train_df['humor_rating']/5).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Agafk2uQ39Cd"},"outputs":[],"source":["import nlpaug.augmenter.word as naw\n","\n","# Your code goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4b6SkXk4Kuu"},"outputs":[],"source":["train_input_ids, train_attention_masks, train_targets = get_bert_inputs(train_examples_list, train_targets_list)\n","test_input_ids, test_attention_masks, test_targets = get_bert_inputs(test_examples_list, test_targets_list)\n"]},{"cell_type":"markdown","metadata":{"id":"PWrhEU4CAiUe"},"source":["We have now augmented the original data twice:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d1m15P3oEh-"},"outputs":[],"source":["print(\"Training examples before augmentation:\")\n","print(len(train_df['text'].tolist()))\n","print(\"Training examples after augmentation:\")\n","print(len(train_examples_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qc15eVYtG2Dv"},"outputs":[],"source":["model = get_model()\n","history = model.fit([train_input_ids,train_attention_masks],\n","                    train_targets,\n","                    epochs=EPOCHS,\n","                    batch_size=BATCH_SIZE,\n","                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGrTvBP9OtMX"},"outputs":[],"source":["results = model.evaluate([np.asarray(test_input_ids),np.asarray(test_attention_masks)], test_targets)\n","print('Test loss:', results[0])\n","print('Test MSE:', results[1])"]},{"cell_type":"markdown","metadata":{"id":"Kpz5hGuIRYBX"},"source":["Your results may be different depending on the implementation but typically there are no drastic differences between the augmentation setups."]},{"cell_type":"markdown","metadata":{"id":"F4RbT-0T3OMY"},"source":["# Ensembled BERT Model\n","\n","In this section you will train and evaluate an **ensemble** of BERT models. \n","\n","We define the hyperparameters, including the number of models we want to ensemble (RERUNS=3, i.e., 3 models). \n","\n","**Task 4: Train three DistilBERT models, get their predictions on the test set, take the mean of those predictions and evaluate this ensembled prediction. Comment on the resulting performance.**\n","\n","We create three models in a loop, set a new random seed before creating each of them (`set_random_seed(seed=random.randint(0, 500))`) and accumulate predictions per model in a list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMIdOlIukwu-"},"outputs":[],"source":["# Get the train data again to avoid any confusion\n","train_examples_list = train_df['text'].tolist()\n","train_targets_list = (train_df['humor_rating']/5).tolist()\n"," \n","train_input_ids, train_attention_masks, train_targets = get_bert_inputs(train_examples_list, train_targets_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PI5nSuK3jygf"},"outputs":[],"source":["RERUNS = 3\n","# We save the predictions of each model to the list\n","all_model_preds = list()\n","\n","for i in range(RERUNS):\n","\n","  set_random_seed(seed=random.randint(0, 500))\n","\n","  # Your code goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zzm8Afa1wz7i"},"outputs":[],"source":["from sklearn.metrics import classification_report, mean_squared_error\n","\n","mean_preds = np.mean(np.array(all_model_preds), axis=0)\n","ensemble_mse = mean_squared_error(test_targets, mean_preds)\n","\n","print('Ensemble Test MSE : {:.4f}'.format(ensemble_mse))"]},{"cell_type":"markdown","metadata":{"id":"UNa2mwVBRzYq"},"source":["Your results may be different depending on the implementation but typically ensembling slightly improves the performance for this task."]},{"cell_type":"markdown","metadata":{"id":"IL2QN-k32OSj"},"source":["# Multi-task Learning with BERT\n","\n","**Task 6: Train a multi-task (MTL) model with the additional regression task of predicting the offense rating. Re-train the single-task model from Task 1 with half of the initial training data. The code to fetch these data is provided below. Comment on the resulting performance for the regresion task in the data sparsity conditions for the two models (single-task and MTL).** \n","\n","*Hint*: The MTL model will have two identical output layers (one for predicting humour rating, the other to predict offense rating).\n","\n","We specify two losses and two metrics to compile the model `loss={'out_reg1': 'mse', 'out_reg2': 'mse'}, metrics={'out_reg1': 'mse', 'out_reg2': 'mse'}`. We increase the epoch count to 25 due to the reduced training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eV5DB1QTSRLm"},"outputs":[],"source":["set_random_seed()\n","\n","from transformers import TFDistilBertModel, DistilBertConfig\n","\n","def create_TFBertMultitask():\n","\n","  # Your code goes here\n","\n","  # comment to run a single-task model\n","  return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = [out_reg1, out_reg2])\n","  # uncomment to run a single-task model\n","  # return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = [out_reg1])\n","\n","use_tpu = True\n","use_gpu = False\n","if use_tpu:\n","  # Create distribution strategy\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","\n","  # Create model on TPU:\n","  with strategy.scope():\n","    model = create_TFBertMultitask()\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n","    # comment to run a single-task model\n","    model.compile(optimizer=optimizer, loss={'out_reg1': 'mse', \n","                         'out_reg2': 'mse'}, metrics={'out_reg1': 'mse', \n","                          'out_reg2': 'mse'})\n","    # uncomment to run a single-task model\n","    # model.compile(optimizer=optimizer, loss={'out_reg1': 'mse'}, metrics={'out_reg1': 'mse'})\n","  \n","elif use_gpu:\n","  device_name = tf.test.gpu_device_name()\n","  print(device_name)\n","  with tf.device('/device:GPU:0'):\n","    model = create_TFBertMultitask()\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n","    model.compile(optimizer=optimizer, loss={'out_reg1': 'mse', \n","                         'out_reg2': 'mse'}, metrics={'out_reg1': 'mse', \n","                          'out_reg2': 'mse'})\n","else:\n","  model = create_TFBertMultitask()\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n","  model.compile(optimizer=optimizer, loss={'out_reg1': 'mse', \n","                         'out_reg2': 'mse'}, metrics={'out_reg1': 'mse', \n","                          'out_reg2': 'mse'})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lfci4A85inT2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677767940865,"user_tz":0,"elapsed":26,"user":{"displayName":"julia ive","userId":"11396706016758891847"}},"outputId":"25694a8a-ae34-4925-abc1-d0027d16e4e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_token (InputLayer)       [(None, 128)]        0           []                               \n","                                                                                                  \n"," masked_token (InputLayer)      [(None, 128)]        0           []                               \n","                                                                                                  \n"," tf_distil_bert_model_6 (TFDist  TFBaseModelOutput(l  66362880   ['input_token[0][0]',            \n"," ilBertModel)                   ast_hidden_state=(N               'masked_token[0][0]']           \n","                                one, 128, 768),                                                   \n","                                 hidden_states=None                                               \n","                                , attentions=None)                                                \n","                                                                                                  \n"," global_average_pooling1d_maske  (None, 768)         0           ['tf_distil_bert_model_6[0][0]'] \n"," d_6 (GlobalAveragePooling1DMas                                                                   \n"," ked)                                                                                             \n","                                                                                                  \n"," dense_6 (Dense)                (None, 16)           12304       ['global_average_pooling1d_masked\n","                                                                 _6[0][0]']                       \n","                                                                                                  \n"," out_reg1 (Dense)               (None, 1)            17          ['dense_6[0][0]']                \n","                                                                                                  \n"," out_reg2 (Dense)               (None, 1)            17          ['dense_6[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 66,375,218\n","Trainable params: 66,375,218\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XQqz3Gr2ZkG"},"outputs":[],"source":["# Get half of the training data\n","train_examples_list = train_df['text'].tolist()[2500:]\n","train_targets_list = (train_df['humor_rating']/5).tolist()[2500:]\n","\n","train_input_ids, train_attention_masks, train_targets = get_bert_inputs(train_examples_list, train_targets_list)\n","\n","# Get the offense ratings for the second regression task (we normalise them as well)\n","train_targets2_list = (train_df['offense_rating']/5).tolist()[2500:]\n","test_targets2_list = (test_df['offense_rating']/5).tolist()\n"," \n","train_targets2 = np.array(train_targets2_list)\n","test_targets2 = np.array(test_targets2_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX7fEExQZ8tE"},"outputs":[],"source":["# comment to run a single-task model\n","\n","history = model.fit([train_input_ids, train_attention_masks],\n","                    [train_targets, train_targets2],\n","                    epochs=25,\n","                    batch_size=BATCH_SIZE,\n","                    verbose=1)\n","\n","# uncomment to run a single-task model\n","# history = model.fit([train_input_ids, train_attention_masks],\n","#                    [train_targets],\n","#                    epochs=25,\n","#                    batch_size=BATCH_SIZE,\n","#                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiJg_LlqaRPp"},"outputs":[],"source":["results = model.evaluate([test_input_ids,test_attention_masks], [test_targets, test_targets2])\n","print('Test loss:', results[0])\n","print('Test MSE:', results[1])"]},{"cell_type":"markdown","metadata":{"id":"ARRZiV9Ga79g"},"source":["Your results may be different depending on the implementation but typically MTL slightly improves the performance over the single-task model trained in similar conditions."]}],"metadata":{"accelerator":"TPU","colab":{"toc_visible":true,"provenance":[{"file_id":"1YbRYkRmy0NzX75u0c-laT-CYVb89V7G_","timestamp":1677772400579}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":0}